# Project1-Software-EC601
Self-supervision for graph data structures  
Study the problem of large data and visual graphs. Experiment with self-supervised models to find relationship between data to display and predict.  
# 1. Introduction to self-supervision
## 1.1 What is self-supervision
Self-supervised learning is a subset of unsupervised learning methods. Self-supervised learning refers to learning methods in which ConvNets are explicitly trained with automatically generated labels. This review only focuses on self-supervised learning methods for visual feature learning with ConvNets in which the features can be transferred to multiple different computer vision tasks.  
## 1.2 Why it is important?
To train deep neural networks, large-scale labeled data are generally required in order to obtain better performance in visual feature learning from images or videos for computer vision applications. To avoid extensive cost of collecting and annotating large-scale datasets, as a subset of unsupervised learning methods, self-supervised learning methods are proposed to learn general image and video features from large-scale unlabeled data without using any human-annotated labels.   
## 1.3 How does it work?--Pretext tasks
To learn visual features from unlabeled data, a popular solution is to propose various pretext tasks for networks to solve, while the networks can be trained by learning objective functions of the pretext tasks and the features are learned through this process. Various pretext tasks have been proposed for self-supervised learning including colorizing grayscale images, image inpainting, image jigsaw puzzle, etc. The pretext tasks share two common properties: (1) visual features of images or videos need to be captured by ConvNets to solve the pretext tasks, (2) pseudo labels for the pretext task can be automatically generated based on the attributes of images or videos.  
![image](https://user-images.githubusercontent.com/78338843/133964841-db670b87-a971-44fb-8ef8-f240d3647edb.png)
During the self-supervised training phase, a pre-defined pretext task is designed for ConvNets to solve, and the pseudo labels for the pretext task are automatically generated based on some attributes of data. Then the ConvNet is trained to learn object functions of the pretext task. After the self-supervised training finished, the learned visual features can be further transferred to downstream tasks (especially when only relatively small data available) as pretrained models to improve performance and overcome over-fitting. Generally, shallow layers capture general low-level features like edges, corners, and textures while deeper layers capture task related high-level features. Therefore, visual features from only the first several layers are transferred during the supervised downstream task training phase.  
# 2. Applications
## 2.1 Colorful Image Colorization
This paper propose a fully automatic approach that produces vibrant and realistic colorizations. They embrace the underlying uncertainty of the problem by posing it as a classification task and use class-rebalancing at training time to increase the diversity of colors in the result. The system is implemented as a feed-forward pass in a CNN at test time and is trained on over a million color images. 
![image](https://user-images.githubusercontent.com/78338843/133965037-995df868-68d0-40bf-bd68-613914c4319a.png)
## 2.2 Placing image patches in the right place
In this paper they study the problem of image representation learning without human annotation. By following the principles of self-supervision, they build a convolutional neural network (CNN) that can be trained to solve Jigsaw puzzles as a pretext task, which requires no manual labeling, and then later repurposed to solve object classification and detection. To maintain the compatibility across tasks they introduce the context-free network (CFN), a siamese-ennead CNN. The CFN takes image tiles as input and explicitly limits the receptive field (or context) of its early processing units to one tile at a time.
![image](https://user-images.githubusercontent.com/78338843/133965605-8abb1179-9df6-4d98-b851-706738b40943.png)
## 2.3 Placing frames in the right order
This project validate the effectiveness of the learned representation using our method as pre-training on high-level recognition problems. The experimental results show that our method compares favorably against state-of-the-art methods on action recognition, image classification and object detection tasks.
![image](https://user-images.githubusercontent.com/78338843/133966077-857bf040-9b29-4193-9829-a296198ba80f.png)
# 3. Open Source research - SimCLR
I search many applications on Google and Github, one open source project is a framework for contrastive learning of visual representations called SimCLR. Which is also discussed by our group on the class last Thursday.  
To begin, SimCLR randomly draws examples from the original dataset, transforming each example twice using a combination of simple augmentations (random cropping, random color distortion, and Gaussian blur), creating two sets of corresponding views. The rationale behind these simple transformations of individual images is (1) encourage "consistent" representation of the same image under transformations, (2) since the pretraining data lacks labels, we can’t know a priori which image contains which object class, and 3) we found that these simple transformations are suffice for the neural net to learn good representations, though more sophisticated transformation policy can also be incorporated.  
SimCLR then computes the image representation using a convolutional neural network variant based on the ResNet architecture. Afterwards, SimCLR computes a non-linear projection of the image representation using a fully-connected network (i.e., MLP), which amplifies the invariant features and maximizes the ability of the network to identify different transformations of the same image. We use stochastic gradient descent to update both CNN and MLP in order to minimize the loss function of the contrastive objective. After pre-training on the unlabeled images, we can either directly use the output of the CNN as the representation of an image, or we can fine-tune it with labeled images to achieve good performance for downstream tasks.  
# 4. Further research
Actually I’m also very interested in many other applications of Self-supervision for graph data structures, such as placing image patches in the right place mentioned in 2.2.  
For further research, I want to spend more time finding more open source research and duplicate their results to make comparison, and find where my interest is.
# 5. References
[1]Longlong Jing and Yingli Tian, Self-supervised Visual Feature Learning with Deep Neural Networks: A Survey, arXiv:1902.06162v1 [cs.CV] 16 Feb 2019. https://arxiv.org/abs/1902.06162    
[2]R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hierarchies for accurate object detection and semantic segmentation,” in CVPR, pp. 580–587, 2014.  
[3]R. Girshick, “Fast R-CNN,” in ICCV, 2015.  
[4]S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards realtime object detection with region proposal networks,” in NIPS, pp. 91–99, 2015.  
[5]J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for semantic segmentation,” in CVPR, pp. 3431–3440, 2015.  
